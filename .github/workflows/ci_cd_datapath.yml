name: CI/CD Airflow DAGs 

on:
  push:
    branches:
      - main  # Ejecuta el pipeline solo en la rama main

jobs:
  deploy:
    runs-on: ubuntu-latest

    steps:
    - name: Checkout repo content
      uses: actions/checkout@v2

    # Verificar si hubo cambios en la carpeta "dags/"
    - name: Check for DAG changes
      id: check_dags
      uses: dorny/paths-filter@v2
      with:
        filters: |
          dags:
            - 'dags/**'

    # Validar DAGs antes del despliegue
    - name: Validate DAGs
      if: steps.check_dags.outputs.dags == 'true'
      run: |
        pip install apache-airflow
        airflow db init  # Inicializar DB para evitar errores
        airflow dags test example_dag $(date +%Y-%m-%d)  # Prueba con fecha actual

    - name: Set up Python
      if: steps.check_dags.outputs.dags == 'true'
      uses: actions/setup-python@v2
      with:
        python-version: '3.8'

    - name: Install SSH key
      if: steps.check_dags.outputs.dags == 'true'
      uses: shimataro/ssh-key-action@v2
      with:
        key: ${{ secrets.SSH_PRIVATE_KEY }}
        known_hosts: ${{ secrets.SSH_KNOWN_HOSTS }}

    # Probar conexión SSH antes del despliegue
    - name: Test SSH Connection
      if: steps.check_dags.outputs.dags == 'true'
      run: |
        ssh -o StrictHostKeyChecking=no your_user@172.29.210.222 "echo 'Conexión SSH exitosa desde GitHub Actions'"

    # Desplegar los DAGs solo si hubo cambios en la carpeta "dags/"
    - name: Copy DAG files to Airflow server
      if: steps.check_dags.outputs.dags == 'true'
      run: |
        scp -r ./dags/* datapath@172.29.210.222:/home/roberthc/airflow/dags

    # Reiniciar Airflow solo si hubo cambios
    - name: Restart Airflow
      if: steps.check_dags.outputs.dags == 'true'
      run: |
        ssh datapath@172.29.210.222 'sudo systemctl restart airflow-scheduler airflow-webserver'
